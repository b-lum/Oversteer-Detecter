{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05eb69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "from transformers import AutoImageProcessor, AutoModelForVideoClassification, AutoConfig\n",
    "import numpy as np \n",
    "\n",
    "from pathlib import Path \n",
    "from datasets import Dataset, Features, Array3D, ClassLabel, Value, load_from_disk\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm.notebook import tqdm\\\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the fine tuning stuff\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e44050cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ccaa1f420e46c4aeeebf5ea6e2075c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dat aset\n",
    "\n",
    "preprocesed_dataset = load_from_disk(\"C:\\\\Users\\\\brand\\\\desktop\\\\workspace\\\\f1-telemetry-app\\\\fine-tune\\\\preprocessed_dataset\")\n",
    "\n",
    "# C:\\Users\\brand\\desktop\\workspace\\f1-telemetry-app\\fine-tune\\preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cdcb7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video shape: {'label': List(Value('int64'), length=3), 'pixel_values': List(List(List(List(Value('float32'))))), 'labels': List(Value('int64'))}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# confirm data set save is correct\n",
    "\n",
    "#first_example = preprocesed_dataset[1]\n",
    "#frames = np.array(first_example[\"features\"])\n",
    "#label = first_example[\"label\"]\n",
    "\n",
    "sample = preprocesed_dataset[0]\n",
    "print(\"Video shape:\", preprocesed_dataset.features)\n",
    "torch.tensor(sample[\"pixel_values\"]).shape\n",
    "\n",
    "#print(\"Label:\", label)\n",
    "\n",
    "# for idx in range(0, len(frames)) :\n",
    "#    plt.imshow(frames[idx])\n",
    "#     plt.title(f\"Label: {label}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "677590c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae1ddfd24784529b0a388b79ea80b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([972]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([972, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "\n",
    "\n",
    "\n",
    "# condfig to overide number of classes\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(preprocesed_dataset[\"label\"])\n",
    "# take raw video frames, resize to right size, convert to pytorch tensors, normalize pixel values\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForVideoClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config = config,\n",
    "    ignore_mismatched_sizes = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad893cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,337,292 || all params: 88,311,960 || trainable%: 1.5143\n"
     ]
    }
   ],
   "source": [
    "# wrap VideoMae model with LoRA\n",
    "\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, # try 4-32\n",
    "    lora_alpha = 32, # scaling factor,\n",
    "    target_modules = [\"query\", \"value\"], # attention layers to apply LoRA to\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    task_type = \"SEQ_CLS\" # find a better way for this\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config) # wrapping model\n",
    "\n",
    "model.print_trainable_parameters() # see trainable parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a8b86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocesed_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1afa05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to collate jagged array\n",
    "\n",
    "def collate_fn(batch, num_frames = 60) :\n",
    "    pixel_values = []\n",
    "    labels = []\n",
    "\n",
    "    for item in batch :\n",
    "        vid = torch.tensor(item[\"pixel_values\"])\n",
    "                           \n",
    "        if vid.shape[0] > num_frames :\n",
    "            vid = vid[:num_frames]\n",
    "        elif vid.shape[0] < num_frames :\n",
    "            pad = num_frames - vid.shape[0]\n",
    "            vid = F.pad(vid, (0,0,0,0,0,0,0,pad))  # pad frames dimension\n",
    "        \n",
    "        pixel_values.append(vid)\n",
    "        labels.append(torch.tensor(item[\"labels\"]).argmax().long())\n",
    "    \n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de819235",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle = True, collate_fn = collate_fn)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = 4, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "335858c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f47049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1568, 768])\n"
     ]
    }
   ],
   "source": [
    "backbone = model.base_model.model.videomae\n",
    "pos_embed = backbone.embeddings.position_embeddings  # shape: [1, old_seq_len, dim]\n",
    "print(pos_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d292990",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = pos_embed.shape[1]  # frames * patches/frame\n",
    "dim = pos_embed.shape[2]\n",
    "\n",
    "num_patches_per_frame = seq_len // 16\n",
    "pos_embed_reshaped = pos_embed.view(1, 16, num_patches_per_frame, dim)  # [1, 16, P, D]\n",
    "\n",
    "pos_embed_reshaped = pos_embed_reshaped.permute(0, 3, 2, 1)             # [1, D, P, 16]\n",
    "pos_embed_reshaped = pos_embed_reshaped.reshape(1, dim * num_patches_per_frame, 16)  # [1, D*P, 16]\n",
    "\n",
    "# interpolate frames from 16 → 60\n",
    "pos_embed_resized = F.interpolate(\n",
    "    pos_embed_reshaped, size=60, mode=\"linear\", align_corners=False\n",
    ")  # [1, D*P, 60]\n",
    "\n",
    "# reshape back\n",
    "pos_embed_resized = pos_embed_resized.view(1, dim, num_patches_per_frame, 60)  \n",
    "# [1, D, P, 60]\n",
    "\n",
    "pos_embed_resized = pos_embed_resized.permute(0, 3, 2, 1)  # [1, 60, P, D]\n",
    "\n",
    "# final flatten\n",
    "pos_embed_new = pos_embed_resized.reshape(1, 60 * num_patches_per_frame, dim)\n",
    "\n",
    "backbone.embeddings.position_embeddings = torch.nn.Parameter(pos_embed_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ec14376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_18256\\4080277884.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values = torch.tensor(batch[\"pixel_values\"]).to(device) # move inputs to gpu\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_18256\\4080277884.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"labels\"]).to(device).long() # move inputs to gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6806, device='cuda:0', grad_fn=<NllLossBackward0>) torch.Size([4, 972])\n",
      "pixel_values: torch.Size([4, 60, 3, 224, 224])\n",
      "labels: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)  # move model to gpu\n",
    "\n",
    "\n",
    "pixel_values = torch.tensor(batch[\"pixel_values\"]).to(device) # move inputs to gpu\n",
    "labels = torch.tensor(batch[\"labels\"]).to(device).long() # move inputs to gpu\n",
    "\n",
    "outputs = model(pixel_values = pixel_values, labels = labels)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "\n",
    "print(\"pixel_values:\", pixel_values.shape)\n",
    "print(\"labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f950377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- HYPERPARAMS ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 8\n",
    "train_batch_size = 4           # actual dataloader batch size\n",
    "eval_batch_size = 4\n",
    "learning_rate = 3e-4           # a good starting point for LoRA / small adapter training\n",
    "weight_decay = 0.01\n",
    "gradient_accumulation_steps = 1  # increase if you need larger effective batch\n",
    "max_grad_norm = 1.0\n",
    "save_dir = \"./checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "use_amp = True                 # mixed precision (recommended when using CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55020cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer: only parameters that require_grad (PEFT typically sets base params frozen)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                  lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d306f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_18256\\374530153.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled = (use_amp and torch.cuda.is_available()))\n"
     ]
    }
   ],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / gradient_accumulation_steps)\n",
    "max_train_steps = epochs * num_update_steps_per_epoch\n",
    "warrmup_steps = int(0.06 * max_train_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warrmup_steps, num_training_steps = max_train_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled = (use_amp and torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5828f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Eval\", leave=False):\n",
    "            # move to device\n",
    "            pixel_values = batch[\"pixel_values\"].to(device).float()\n",
    "            labels = batch[\"labels\"].to(device).long()   # already class indices now\n",
    "\n",
    "            # forward (model returns .loss when labels provided)\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = logits.argmax(dim=1)   # [B]\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss_sum += loss.item() * labels.size(0)\n",
    "\n",
    "    avg_loss = loss_sum / total if total > 0 else 0.0\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    model.train()\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e2b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4401f0e387494d7cbe1309e3ae3e90be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/8:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_18256\\409661644.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(use_amp and torch.cuda.is_available())):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5635b6df07584703ac79b2b47fb3e55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished — train_loss: 2.7715 — val_loss: 0.6009 — val_acc: 0.7653\n",
      "New best val acc: 0.7653. Saving checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df09d9007004d92948e0a7567b737ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/8:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d927b4b3aa4d11894cba2cdd90b375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished — train_loss: 0.6469 — val_loss: 0.5053 — val_acc: 0.7959\n",
      "New best val acc: 0.7959. Saving checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187405d45cf24f5f9b42deb1a82dcc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/8:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- TRAIN LOOP ----------\n",
    "global_step = 0\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()                           # put model in training mode\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(pbar):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device).float()  # ensure float\n",
    "        labels = batch[\"labels\"].to(device).long()               # ensure long\n",
    "\n",
    "        # mixed precision context\n",
    "        with torch.cuda.amp.autocast(enabled=(use_amp and torch.cuda.is_available())):\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            # if using gradient accumulation, scale loss down here\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        # scale the loss, backprop with scaler (AMP)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # gradient accumulation step\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # unscale before clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            # optional gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), max_grad_norm)\n",
    "\n",
    "            # step optimizer and scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # scheduler step (after optimizer.step)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "        epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "        pbar.set_postfix({\"avg_loss\": epoch_loss / (step + 1)})\n",
    "\n",
    "    # --- end epoch: evaluate ---\n",
    "    val_loss, val_acc = evaluate(model, eval_loader, device)\n",
    "    print(f\"Epoch {epoch+1} finished — train_loss: {epoch_loss/len(train_loader):.4f} — val_loss: {val_loss:.4f} — val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    # save checkpoint if improved\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print(f\"New best val acc: {best_val_acc:.4f}. Saving checkpoint...\")\n",
    "        # 1) save full state (model + optimizer + scheduler) for resuming\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"scaler_state_dict\": scaler.state_dict()\n",
    "        }, os.path.join(save_dir, \"best_full.pt\"))\n",
    "\n",
    "        # 2) save the PEFT adapters (recommended) - this saves LoRA adapter config & weights\n",
    "        # model here is a PeftModel; this will save the adapter so you can re-apply it to base later\n",
    "        model.save_pretrained(os.path.join(save_dir, \"peft_adapter\"))\n",
    "\n",
    "# after training you can save the final model weights / adapter again:\n",
    "model.save_pretrained(os.path.join(save_dir, \"final_peft_adapter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d7bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss, loss.requires_grad, loss.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403bc29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
